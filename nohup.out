Using TensorFlow backend.
WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.

WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/qbeer666/.local/share/virtualenvs/wigner-csnl-textures-nHVgIMTt/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

2019-10-31 16:12:28.881547: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-10-31 16:12:28.886714: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-10-31 16:12:29.006278: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63a87a0 executing computations on platform CUDA. Devices:
2019-10-31 16:12:29.006320: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
2019-10-31 16:12:29.009245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3599260000 Hz
2019-10-31 16:12:29.009740: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x641a6c0 executing computations on platform Host. Devices:
2019-10-31 16:12:29.009770: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2019-10-31 16:12:29.014556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.7465
pciBusID: 0000:01:00.0
2019-10-31 16:12:29.014979: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-31 16:12:29.016613: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-10-31 16:12:29.017988: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-10-31 16:12:29.018399: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-10-31 16:12:29.020298: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-10-31 16:12:29.021669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-10-31 16:12:29.025431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-10-31 16:12:29.026928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-10-31 16:12:29.026991: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-10-31 16:12:29.028419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-10-31 16:12:29.028446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-10-31 16:12:29.028461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-10-31 16:12:29.031983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7191 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-10-31 16:12:30.531443: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
Training size : 37800 	 Test size : 4200
Shapes :  (37800, 28, 28, 1) 	 (4200, 28, 28, 1)
Label shaped :  (37800,) 	 (4200,)
Train set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Test set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Train set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Train SHAPE :  (37800, 28, 28, 1)
Test SHAPE :  (4200, 28, 28, 1)
Training size : 37800 	 Test size : 4200
Shapes :  (37800, 28, 28, 1) 	 (4200, 28, 28, 1)
Label shaped :  (37800,) 	 (4200,)
Train set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Test set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Train set : 
Mean: 0.499, Standard Deviation: 0.157
Min: 0.000, Max: 1.000
Train SHAPE :  (37800, 28, 28, 1)
Test SHAPE :  (4200, 28, 28, 1)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (100, 784)           0                                            
__________________________________________________________________________________________________
dense_encoder_1 (Model)         multiple             1476416     input_1[0][0]                    
__________________________________________________________________________________________________
dense_encoder_2 (Model)         multiple             230400      dense_encoder_1[1][0]            
__________________________________________________________________________________________________
mean_z2 (Dense)                 (100, 8)             2056        dense_encoder_2[1][0]            
__________________________________________________________________________________________________
log_sigma_z2 (Dense)            (100, 8)             2056        dense_encoder_2[1][0]            
__________________________________________________________________________________________________
sampling_z2 (Lambda)            (100, 8)             0           mean_z2[0][0]                    
                                                                 log_sigma_z2[0][0]               
__________________________________________________________________________________________________
dense_decoder_2 (Model)         multiple             265216      sampling_z2[0][0]                
__________________________________________________________________________________________________
mean_log_sigma_model (Model)    multiple             32896       dense_decoder_2[1][0]            
__________________________________________________________________________________________________
bottom_up_mean (Dense)          (100, 64)            4160        dense_encoder_1[1][0]            
__________________________________________________________________________________________________
bottom_up_log_sigma (Dense)     (100, 64)            4160        dense_encoder_1[1][0]            
__________________________________________________________________________________________________
calculate_mean_z1 (Lambda)      (100, 64)            0           mean_log_sigma_model[1][0]       
                                                                 mean_log_sigma_model[1][1]       
                                                                 bottom_up_mean[0][0]             
                                                                 bottom_up_log_sigma[0][0]        
__________________________________________________________________________________________________
calculate_sigma_z1 (Lambda)     (100, 64)            0           bottom_up_log_sigma[0][0]        
                                                                 mean_log_sigma_model[1][1]       
__________________________________________________________________________________________________
sampling_z1 (Lambda)            (100, 64)            0           calculate_mean_z1[0][0]          
                                                                 calculate_sigma_z1[0][0]         
__________________________________________________________________________________________________
dense_decoder_1 (Model)         multiple             4264208     sampling_z1[0][0]                
==================================================================================================
Total params: 6,281,568
Trainable params: 6,281,568
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100

  1/500 [..............................] - ETA: 15:25 - loss: 10830489600.0000 - KL_divergence: 0.0000e+00
  6/500 [..............................] - ETA: 2:37 - loss: 6449488853.3333 - KL_divergence: 0.0000e+00  
 10/500 [..............................] - ETA: 1:35 - loss: 4553325030.4000 - KL_divergence: 0.0000e+00
 15/500 [..............................] - ETA: 1:05 - loss: 3509956002.1333 - KL_divergence: 0.0000e+00
 20/500 [>.............................] - ETA: 49s - loss: 2953545792.0000 - KL_divergence: 0.0000e+00 
 25/500 [>.............................] - ETA: 40s - loss: 2619690449.9200 - KL_divergence: 0.0000e+00
 30/500 [>.............................] - ETA: 34s - loss: 2404497292.8000 - KL_divergence: 0.0000e+00
 35/500 [=>............................] - ETA: 29s - loss: 2244078935.7714 - KL_divergence: 0.0000e+00
 40/500 [=>............................] - ETA: 26s - loss: 2104236515.2000 - KL_divergence: 0.0000e+00
 45/500 [=>............................] - ETA: 23s - loss: 1998537291.3778 - KL_divergence: 0.0000e+00
 50/500 [==>...........................] - ETA: 21s - loss: 1920895991.0400 - KL_divergence: 0.0000e+00
 55/500 [==>...........................] - ETA: 19s - loss: 1845654120.7273 - KL_divergence: 0.0000e+00
 61/500 [==>...........................] - ETA: 17s - loss: 1777848633.7049 - KL_divergence: 0.0000e+00
 67/500 [===>..........................] - ETA: 16s - loss: 1725290602.9851 - KL_divergence: 0.0000e+00
 73/500 [===>..........................] - ETA: 15s - loss: 1677609996.2740 - KL_divergence: 0.0000e+00
 79/500 [===>..........................] - ETA: 14s - loss: 1633988057.1139 - KL_divergence: 0.0000e+00
 85/500 [====>.........................] - ETA: 13s - loss: 1600680140.0471 - KL_divergence: 0.0000e+00
 90/500 [====>.........................] - ETA: 12s - loss: 1574722937.6000 - KL_divergence: 0.0000e+00
 95/500 [====>.........................] - ETA: 11s - loss: 1550864339.5368 - KL_divergence: 0.0000e+00
101/500 [=====>........................] - ETA: 11s - loss: 1524337840.1584 - KL_divergence: 0.0000e+00
106/500 [=====>........................] - ETA: 10s - loss: 1501657874.1132 - KL_divergence: 0.0000e+00
112/500 [=====>........................] - ETA: 10s - loss: 1484716334.8571 - KL_divergence: 0.0000e+00
118/500 [======>.......................] - ETA: 9s - loss: 1465920234.3051 - KL_divergence: 0.0000e+00 
123/500 [======>.......................] - ETA: 9s - loss: 1451792958.4390 - KL_divergence: 0.0000e+00
128/500 [======>.......................] - ETA: 9s - loss: 1438259492.0000 - KL_divergence: 0.0000e+00
133/500 [======>.......................] - ETA: 8s - loss: 1423928213.1729 - KL_divergence: 0.0000e+00
138/500 [=======>......................] - ETA: 8s - loss: 1411894679.1884 - KL_divergence: 0.0000e+00
143/500 [=======>......................] - ETA: 8s - loss: 1400504575.1049 - KL_divergence: 0.0000e+00
148/500 [=======>......................] - ETA: 7s - loss: 1389266825.0811 - KL_divergence: 0.0000e+00
153/500 [========>.....................] - ETA: 7s - loss: 1380053191.9477 - KL_divergence: 0.0000e+00
158/500 [========>.....................] - ETA: 7s - loss: 1370983903.5949 - KL_divergence: 0.0000e+00
163/500 [========>.....................] - ETA: 7s - loss: 1365610686.8221 - KL_divergence: 0.0000e+00
168/500 [=========>....................] - ETA: 7s - loss: 1358363469.3333 - KL_divergence: 0.0000e+00
173/500 [=========>....................] - ETA: 6s - loss: 1351433271.1214 - KL_divergence: 0.0000e+00
178/500 [=========>....................] - ETA: 6s - loss: 1343985827.2360 - KL_divergence: 0.0000e+00
183/500 [=========>....................] - ETA: 6s - loss: 1337811705.7049 - KL_divergence: 0.0000e+00
189/500 [==========>...................] - ETA: 6s - loss: 1328764504.0423 - KL_divergence: 0.0000e+00
195/500 [==========>...................] - ETA: 6s - loss: 1322491590.2359 - KL_divergence: 0.0000e+00
201/500 [===========>..................] - ETA: 5s - loss: 1316315485.9303 - KL_divergence: 0.0000e+00
207/500 [===========>..................] - ETA: 5s - loss: 1312334471.7295 - KL_divergence: 0.0000e+00
213/500 [===========>..................] - ETA: 5s - loss: 1306163529.6150 - KL_divergence: 0.0000e+00
219/500 [============>.................] - ETA: 5s - loss: 1299062076.2009 - KL_divergence: 0.0000e+00
225/500 [============>.................] - ETA: 5s - loss: 1292266441.1022 - KL_divergence: 0.0000e+00
231/500 [============>.................] - ETA: 4s - loss: 1285633845.4719 - KL_divergence: 0.0000e+00
236/500 [=============>................] - ETA: 4s - loss: 1280009194.5763 - KL_divergence: 0.0000e+00
242/500 [=============>................] - ETA: 4s - loss: 1274541551.8678 - KL_divergence: 0.0000e+00
247/500 [=============>................] - ETA: 4s - loss: 1271246210.8502 - KL_divergence: 0.0000e+00
253/500 [==============>...............] - ETA: 4s - loss: 1267834798.0395 - KL_divergence: 0.0000e+00
258/500 [==============>...............] - ETA: 4s - loss: 1262045630.0155 - KL_divergence: 0.0000e+00
264/500 [==============>...............] - ETA: 4s - loss: 1257956545.6970 - KL_divergence: 0.0000e+00
270/500 [===============>..............] - ETA: 3s - loss: 1253900881.7778 - KL_divergence: 0.0000e+00
276/500 [===============>..............] - ETA: 3s - loss: 1249118982.9565 - KL_divergence: 0.0000e+00
282/500 [===============>..............] - ETA: 3s - loss: 1245099145.5319 - KL_divergence: 0.0000e+00
288/500 [================>.............] - ETA: 3s - loss: 1242743473.7778 - KL_divergence: 0.0000e+00
294/500 [================>.............] - ETA: 3s - loss: 1239477847.5102 - KL_divergence: 0.0000e+00
300/500 [=================>............] - ETA: 3s - loss: 1236140193.2800 - KL_divergence: 0.0000e+00
306/500 [=================>............] - ETA: 3s - loss: 1233505661.9085 - KL_divergence: 0.0000e+00
312/500 [=================>............] - ETA: 3s - loss: 1230253251.2821 - KL_divergence: 0.0000e+00
318/500 [==================>...........] - ETA: 2s - loss: 1226860167.2453 - KL_divergence: 0.0000e+00
323/500 [==================>...........] - ETA: 2s - loss: 1225213639.7276 - KL_divergence: 0.0000e+00
329/500 [==================>...........] - ETA: 2s - loss: 1222580069.5441 - KL_divergence: 0.0000e+00
335/500 [===================>..........] - ETA: 2s - loss: 1218881543.2597 - KL_divergence: 0.0000e+00
341/500 [===================>..........] - ETA: 2s - loss: 1215568707.5660 - KL_divergence: 0.0000e+00
348/500 [===================>..........] - ETA: 2s - loss: 1212043178.4828 - KL_divergence: 0.0000e+00
354/500 [====================>.........] - ETA: 2s - loss: 1208161658.2147 - KL_divergence: 0.0000e+00
359/500 [====================>.........] - ETA: 2s - loss: 1204505207.6212 - KL_divergence: 0.0000e+00
364/500 [====================>.........] - ETA: 2s - loss: 1202735301.2747 - KL_divergence: 0.0000e+00
370/500 [=====================>........] - ETA: 1s - loss: 1199419851.4162 - KL_divergence: 0.0000e+00
376/500 [=====================>........] - ETA: 1s - loss: 1197274229.2766 - KL_divergence: 0.0000e+00
382/500 [=====================>........] - ETA: 1s - loss: 1195169455.0785 - KL_divergence: 0.0000e+00
388/500 [======================>.......] - ETA: 1s - loss: 1192975373.0309 - KL_divergence: 0.0000e+00
393/500 [======================>.......] - ETA: 1s - loss: 1191877259.5623 - KL_divergence: 0.0000e+00
398/500 [======================>.......] - ETA: 1s - loss: 1189098234.0503 - KL_divergence: 0.0000e+00
404/500 [=======================>......] - ETA: 1s - loss: 1187643745.4257 - KL_divergence: 0.0000e+00
410/500 [=======================>......] - ETA: 1s - loss: 1185597118.4390 - KL_divergence: 0.0000e+00
416/500 [=======================>......] - ETA: 1s - loss: 1184065568.7692 - KL_divergence: 0.0000e+00
422/500 [========================>.....] - ETA: 1s - loss: 1182523595.3744 - KL_divergence: 0.0000e+00
428/500 [========================>.....] - ETA: 1s - loss: 1180211533.6075 - KL_divergence: 0.0000e+00
433/500 [========================>.....] - ETA: 0s - loss: 1178983705.2748 - KL_divergence: 0.0000e+00
439/500 [=========================>....] - ETA: 0s - loss: 1176324407.1071 - KL_divergence: 0.0000e+00
445/500 [=========================>....] - ETA: 0s - loss: 1174788238.9573 - KL_divergence: 0.0000e+00
451/500 [==========================>...] - ETA: 0s - loss: 1172850322.0222 - KL_divergence: 0.0000e+00
456/500 [==========================>...] - ETA: 0s - loss: 1170764522.1053 - KL_divergence: 0.0000e+00
461/500 [==========================>...] - ETA: 0s - loss: 1169896290.4295 - KL_divergence: 0.0000e+00
466/500 [==========================>...] - ETA: 0s - loss: 1168441305.8197 - KL_divergence: 0.0000e+00
472/500 [===========================>..] - ETA: 0s - loss: 1167278879.7288 - KL_divergence: 0.0000e+00
478/500 [===========================>..] - ETA: 0s - loss: 1166114591.3305 - KL_divergence: 0.0000e+00
483/500 [===========================>..] - ETA: 0s - loss: 1164829612.7867 - KL_divergence: 0.0000e+00
489/500 [============================>.] - ETA: 0s - loss: 1162730706.7157 - KL_divergence: 0.0000e+00
494/500 [============================>.] - ETA: 0s - loss: 1162190493.0202 - KL_divergence: 0.0000e+00
500/500 [==============================] - 7s 14ms/step - loss: 1159737841.6640 - KL_divergence: 0.0000e+00 - val_loss: 1035083372.1905 - val_KL_divergence: 0.0000e+00
Epoch 2/100

  1/500 [..............................] - ETA: 4s - loss: 1164232832.0000 - KL_divergence: 0.0983
  7/500 [..............................] - ETA: 4s - loss: 1071604617.1429 - KL_divergence: 0.0912
 13/500 [..............................] - ETA: 4s - loss: 1036562840.6154 - KL_divergence: 0.0913
 18/500 [>.............................] - ETA: 4s - loss: 1049370972.4444 - KL_divergence: 0.0912
 23/500 [>.............................] - ETA: 4s - loss: 1053849349.5652 - KL_divergence: 0.0915
 29/500 [>.............................] - ETA: 4s - loss: 1048332934.6207 - KL_divergence: 0.0910
 35/500 [=>............................] - ETA: 4s - loss: 1050368952.6857 - KL_divergence: 0.0912
 41/500 [=>............................] - ETA: 4s - loss: 1055295820.4878 - KL_divergence: 0.0908
 47/500 [=>............................] - ETA: 4s - loss: 1049854544.3404 - KL_divergence: 0.0907
 53/500 [==>...........................] - ETA: 4s - loss: 1050049304.1509 - KL_divergence: 0.0907
 59/500 [==>...........................] - ETA: 4s - loss: 1051780005.9661 - KL_divergence: 0.0906
 65/500 [==>...........................] - ETA: 4s - loss: 1050511354.0923 - KL_divergence: 0.0905
 72/500 [===>..........................] - ETA: 4s - loss: 1050921033.7778 - KL_divergence: 0.0903
 77/500 [===>..........................] - ETA: 4s - loss: 1045719094.0260 - KL_divergence: 0.0903
 83/500 [===>..........................] - ETA: 3s - loss: 1046142628.2410 - KL_divergence: 0.0903
 89/500 [====>.........................] - ETA: 3s - loss: 1049541721.8876 - KL_divergence: 0.0901
 95/500 [====>.........................] - ETA: 3s - loss: 1047784677.0526 - KL_divergence: 0.0900
100/500 [=====>........................] - ETA: 3s - loss: 1046758585.6000 - KL_divergence: 0.0899
106/500 [=====>........................] - ETA: 3s - loss: 1047283216.9057 - KL_divergence: 0.0897
112/500 [=====>........................] - ETA: 3s - loss: 1045664779.4286 - KL_divergence: 0.0896
118/500 [======>.......................] - ETA: 3s - loss: 1043393011.5254 - KL_divergence: 0.0895
123/500 [======>.......................] - ETA: 3s - loss: 1040628822.3740 - KL_divergence: 0.0894
129/500 [======>.......................] - ETA: 3s - loss: 1041407245.8915 - KL_divergence: 0.0893
135/500 [=======>......................] - ETA: 3s - loss: 1042412392.7704 - KL_divergence: 0.0892
141/500 [=======>......................] - ETA: 3s - loss: 1041374877.9574 - KL_divergence: 0.0890